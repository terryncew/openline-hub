{
  "receipt_version": "1.5-P",
  "version": "OLR/1.5-P",
  "issued_at": "2025-11-02T00:00:05Z",
  "attrs": {"status": "check"},
  "model": {"family": "demo-llm"},
  "precision": {"train_dtype": "fp16", "infer_dtype": "bf16"},
  "point": "Fallback: fp16 weights, bf16 inference for long prompts.",
  "because": "bf16 reduces overflow without retraining.",
  "but": "Minor throughput penalty vs fp16 on some cards.",
  "so": "Use for long-context tests; swap back to fp16 for quick demos."
}
